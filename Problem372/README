Problem 372:

Originally, the program ran on all (10*9 - 2*10^6)^2 = 996.004*10^12 elements, 
which was too many to compute in any reasonable amount of time. Some alternative
solutions to speed this up were to cut the elements in half, effectvely speeding
it up by 2x, and another was to estimate the answer by finding the ratio of odd to
even values on a smaller data set. I found this ratio to be around 0.32, but it was
not accurate enough to provide a correct answer. Furthermore, this ratio seemed to
vary to be less than or greater than 0.32 as the data size increased, which leads
me to belive that it will not converge to a trivial value. However, the ratio never
strayed too far from 0.32, which was interesting nontheless.

To compare the runtimes, a small dataset with a range of (0, 100000) was used, these
are displayed in a table included in the .zip file

Unfortunately, I could not get the program to finish on the full datasize. Further
optimization would need to be done to get a precice answer any faster, and such a 
solution would rely on the geometry or algebra of the problem, which is difficult
to determine. The process did not get to run for its full 24 hour time slot, 
around hour 18 it received an error "ORTE has lost communication with a remote 
daemon." The full error is in the .log file included in the zip file. 

As far as arithmetic complexity is concerned, splitting the data between the 
processes takes Big_Theta(nprocs * rowsize/nprocs) operations = Big_Theta(rowsize) -> 
Big_Theta(N), where 
rowsize = (10*9 - 2*10^6) for the full data size
And the actual computations for determining the result require Big_theta((rowsize^2) * 8) = 
Big_Theta((N^2)/8) = O(N^2)


Problem 679:

This problem proved to be too difficult for us to figure out with the time alloted.
We figured out a brute force method to solve it, however it would have taken
even longer than the pencils of rays problem to solve. We wrote python scripts
to determine all of the strings we would need to compute on, but considering the 
length of these strings and how many there were, we decided to continue to focus 
on our own problems. 

There was an algorithm for this problem that we wrote that we're actually sort of proud of,
this is in the distStars.py file, included in the zip. It distributes the number
of stars (as in a regular expression, where a number represents how many wildcard
characters can be placed inbetween the characters of the full string), and calculates
all of the distributions while keeping all of them unique. Permutation of these strings 
was also required for our (incomplete) implementation of this problem, and is in 
permuteTest.c. Ideally, this file would do a permutation of all of the strings created 
by distStars.py.

So although much of the work required to finish this problem is yet to be done, 
parallelizing it would require only a few hours of mental labor (as much of it
has been done already) and very, very many computer-hours

